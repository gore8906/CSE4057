{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juUE4GMU4jnB"
      },
      "source": [
        "# **Base Model Part**\n",
        "- **Model:** Llama-2-7b-hf(LLaMA2)\n",
        "- base model version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3B10q2gIzvc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import os\n",
        "\n",
        "login(\"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\")\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    torch_dtype=torch.float16  # loda our model by FP16 mode\n",
        ").to(torch_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S53vwDg6ogrY"
      },
      "source": [
        "**Test Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBmd8QGioc7J"
      },
      "outputs": [],
      "source": [
        "# Define 3 candidate sentences\n",
        "candidate_sentences = [\n",
        "    'Hanyang University is',\n",
        "    'Difficulty of learning Korean compare to English is',\n",
        "    'Alpine ski is'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5db5hANWb8Z"
      },
      "source": [
        "**1. Greedy Search**\n",
        "\n",
        "- It is simplest decoding method.\n",
        "- Selects the highest probability as its next word **at each timestep t**.\n",
        "\n",
        "\n",
        "**Pro**\n",
        "- Simple and fast\n",
        "\n",
        "\n",
        "**Con**\n",
        "- Repetition Happen\n",
        "- Miss high probability words hidden begind a low probability word -> We can alleviate by using **Beam Search**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONVJjUq_WdvM"
      },
      "outputs": [],
      "source": [
        "print(\"Output for Greedy Search:\\n\" + 100 * '-')\n",
        "\n",
        "# generate max 30 new tokens\n",
        "for sentence in candidate_sentences:\n",
        "    # Encode the sentence\n",
        "    model_inputs = tokenizer(sentence, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Generate up to 30 new tokens\n",
        "    greedy_output = model.generate(**model_inputs, max_new_tokens=30)\n",
        "\n",
        "    # Decode the generated text and format output\n",
        "    generated_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the input sentence and the corresponding output\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    print(f\"Generated Output: {generated_text}\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uhriVLyW9U3"
      },
      "source": [
        "**2. Beam Search**\n",
        "\n",
        "- On each step of decoder, keep track of the k most probable partial translations.\n",
        "(k : beam size)\n",
        "\n",
        "- All hypothesis has score(log probability)\n",
        "\n",
        "- Decode untill **reach timestep T** or **at least n completed hypothesis**\n",
        "\n",
        "**Pro**\n",
        "- Reduces the risk of missing hidden high probability word sequences\n",
        "- Work well in task such as Machine Translation or Text Summarization\n",
        "\n",
        "\n",
        "**Con**\n",
        "- Does not guarantee to find optimal solution(or most likely output)\n",
        "- Repetition Happen -> we can solve it by **n-gram**\n",
        "- Not good at open-ended generation such as dialog and story generatiion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfUfpc-Sa-Jj"
      },
      "outputs": [],
      "source": [
        "# activate beam search and early_stopping\n",
        "print(\"Output for Beam Search with Early Stopping:\\n\" + 100 * '-')\n",
        "\n",
        "# Generate max 30 new tokens using beam search\n",
        "for sentence in candidate_sentences:\n",
        "    # Encode the sentence\n",
        "    model_inputs = tokenizer(sentence, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Generate output using beam search\n",
        "    beam_output = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=30,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the input sentence and the corresponding beam search output\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    print(f\"Generated Output (Beam Search): {generated_text}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2RoE1nabrI3"
      },
      "source": [
        "**3. No Repeat N-Gram**\n",
        "\n",
        "- Simple option for not to generate repetition on Beam Search Decoding\n",
        "- N-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPQ597dQb9M_"
      },
      "outputs": [],
      "source": [
        "# set return_num_sequences > 1 for generating multiple sequences\n",
        "print(\"Output for Beam Search with Multiple Sequences:\\n\" + 100 * '-')\n",
        "\n",
        "# Generate max 30 new tokens using beam search with multiple return sequences\n",
        "for sentence in candidate_sentences:\n",
        "    # Encode the sentence\n",
        "    model_inputs = tokenizer(sentence, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Generate output with multiple sequences\n",
        "    beam_outputs = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=30,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Print each generated sequence with index\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    for i, beam_output in enumerate(beam_outputs):\n",
        "        generated_text = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "        print(f\"Sequence {i + 1}: {generated_text}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJBW4QL3cfA8"
      },
      "source": [
        "**4. Vanilla Sampling**\n",
        "\n",
        "- Randomly picking the next word wt according to its conditional probability distribution\n",
        "\n",
        "\n",
        "**Pro**\n",
        "- Repetitiion is not occuring as much as in Beam Search or Greedy Decoding Algorithms\n",
        "\n",
        "**Con**\n",
        "- Model often generate incoherent gibberish beacuse Vanilla sampling makes every token in the vocabulary an option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2gicJFFfj86"
      },
      "outputs": [],
      "source": [
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "print(\"Output for Sampling with Top_k=0:\\n\" + 100 * '-')\n",
        "\n",
        "# Generate max 30 new tokens using sampling with top_k set to 0\n",
        "for sentence in candidate_sentences:\n",
        "    # Encode the sentence\n",
        "    model_inputs = tokenizer(sentence, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Generate output using sampling\n",
        "    sample_output = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        top_k=0\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the input sentence and the corresponding sampled output\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    print(f\"Generated Output (Sampling): {generated_text}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJL968wrgiZ7"
      },
      "source": [
        "**5. Top - K Sampling**\n",
        "\n",
        "- Sample from the top k tokens in the probability distribution.\n",
        "- Incresing k -> Diverse, risky output\n",
        "- Decresing k -> Safe, generic output\n",
        "\n",
        "**Pro**\n",
        "- Much more human like text compare to **vanila Sampling**\n",
        "\n",
        "**Con**\n",
        "- Cannot dynamically adapt the k value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgPSHTajiarL"
      },
      "outputs": [],
      "source": [
        "# set top_k to 50 for sampling\n",
        "print(\"Output for Sampling with Top_k=50:\\n\" + 100 * '-')\n",
        "\n",
        "# Generate max 30 new tokens using sampling with top_k set to 50\n",
        "for sentence in candidate_sentences:\n",
        "    # Encode the sentence\n",
        "    model_inputs = tokenizer(sentence, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Generate output using sampling with top_k set to 50\n",
        "    sample_output = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        top_k=50\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the input sentence and the corresponding sampled output\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    print(f\"Generated Output (Sampling, Top_k=50): {generated_text}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ot4UERjjx9e"
      },
      "source": [
        "**6. Top - p (Nucleus) Sampling**\n",
        "\n",
        "- Sample from all tokens in the top p cumulative probability mass\n",
        "\n",
        "**Pro**\n",
        "- Number of words in the set can dynamically increase and decrese\n",
        "- Diversity increses\n",
        "\n",
        "**Con**\n",
        "- Sensitive to p value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ5ShaK2kUwI"
      },
      "outputs": [],
      "source": [
        "# set top_k to 0 and top_p to 0.92 for sampling\n",
        "print(\"Output for Sampling with Top_k=0 and Top_p=0.92:\\n\" + 100 * '-')\n",
        "\n",
        "# Generate max 30 new tokens using sampling with top_p=0.92 and top_k=0\n",
        "for sentence in candidate_sentences:\n",
        "    # Encode the sentence\n",
        "    model_inputs = tokenizer(sentence, return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # Generate output using sampling with top_p=0.92 and top_k=0\n",
        "    sample_output = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        top_k=0\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the input sentence and the corresponding sampled output\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    print(f\"Generated Output (Sampling, Top_p=0.92, Top_k=0): {generated_text}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o14aooZCxMoB"
      },
      "source": [
        "**Test complex question by Top - p Decoding Alogorithm**\n",
        "\n",
        "- **Complex step by step Question:** \"Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhfwMoeOxVJK"
      },
      "outputs": [],
      "source": [
        "# Test sentence\n",
        "complex_questions =\"\"\"\n",
        "Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Generate max 100 new tokens using sampling with top_p=0.92 and top_k=0\n",
        "\n",
        "# Encode the sentence\n",
        "base_model_inputs = tokenizer(complex_questions, return_tensors='pt').to(torch_device)\n",
        "\n",
        "# Generate output using sampling with top_p=0.92 and top_k=0\n",
        "base_model_output = model.generate(\n",
        "    **base_model_inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(base_model_output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0hhQ508p7ql"
      },
      "source": [
        "**Prompting**\n",
        "- Few Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU5403xI68U1"
      },
      "outputs": [],
      "source": [
        "# Few-Shot Prompt\n",
        "few_shot_prompt = \"\"\"\n",
        "Question1: What is 6 - 4 ?\n",
        "Answer1: Answer is 2.\n",
        "\n",
        "Question2: Alex has 5 pens, and he buys 3 more pens. How many pens does Alex have now?\n",
        "Answer2: Alex now has 8 pens.\n",
        "\n",
        "Question3: Kim has 10 apples and gives 4 apples to Jung. How many apples does Kim have now?\n",
        "Answer3: Kim has 6 apples left.\n",
        "\n",
        "Quserion4: Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\n",
        "\n",
        "What is the answer of Question4?\n",
        "Answer4:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "base_model_few_shot_prompt = tokenizer(few_shot_prompt, return_tensors=\"pt\").to(torch_device)\n",
        "\n",
        "# Generate response\n",
        "base_model_few_shot_output = model.generate(\n",
        "    **base_model_few_shot_prompt,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "few_shot_generated_text = tokenizer.decode(base_model_few_shot_output[0], skip_special_tokens=True)\n",
        "print(few_shot_generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeubiVH28Vlf"
      },
      "source": [
        "**Promptiong**\n",
        "- Chain-of-Thought(COT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdG0ODoo8eQM"
      },
      "outputs": [],
      "source": [
        "# COT Prompt\n",
        "cot_prompt = \"\"\"\n",
        "Question1:Question1: Sarah has 100 dollars. She buys a book for 40 dollars and a pencil for 5 dollars.\n",
        "How much money does Sarah have left?\n",
        "Answer1: Let's go through this step-by-step.\n",
        "Sarah starts with 100 dollars. After buying the book for 40 dollars, she has 100 - 40 = 60 dollars.\n",
        "Then, after buying the pencil for 5 dollars, she has 60 - 5 = 55 dollars. So, Sarah has 55 dollars left.\n",
        "\n",
        "Question2: John’s flight departs at 3:00 PM and arrives at 6:30 PM. How long is John’s flight?\n",
        "Answer2: Let's go through this step-by-step.\n",
        "John’s flight departs at 3:00 PM and arrives at 6:30 PM. From 3:00 PM to 6:00 PM is 3 hours.\n",
        "From 6:00 PM to 6:30 PM is 30 minutes. So, the total flight duration is 3 hours and 30 minutes.\n",
        "\n",
        "Question3: Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\n",
        "\n",
        "What is the answer of Question3?\n",
        "Answer3:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "base_model_cot_prompt = tokenizer(cot_prompt, return_tensors=\"pt\").to(torch_device)\n",
        "\n",
        "# Generate response\n",
        "base_model_cot_output = model.generate(\n",
        "    **base_model_cot_prompt,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "cot_generated_text = tokenizer.decode(base_model_cot_output[0], skip_special_tokens=True)  # 출력도 base_model_cot_output 사용\n",
        "print(cot_generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw86rjdO8Wlc"
      },
      "source": [
        "# **Instruction fine-tuned part**\n",
        "\n",
        "- **Model:** Llama-2-7b-chat-hf(LLaMA2)\n",
        "- Instruction fine-tuned version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmnN_mi78ekO"
      },
      "outputs": [],
      "source": [
        "#Load tokenizer and model\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    torch_dtype=torch.float16  # loda our model by FP16 mode\n",
        ").to(torch_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA5ZzjK58rRI"
      },
      "source": [
        "**Test complex question by Top - p Decoding Alogorithm**\n",
        "\n",
        "- **Complex step by step question:** \"Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e83CorDy9RHt"
      },
      "outputs": [],
      "source": [
        "# Test sentence\n",
        "complex_questions =\"\"\"\n",
        "Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Generate max 100 new tokens using sampling with top_p=0.92 and top_k=0\n",
        "\n",
        "# Encode the sentence\n",
        "fine_tuned_model_inputs = fine_tuned_tokenizer(complex_questions, return_tensors='pt').to(torch_device)\n",
        "\n",
        "# Generate output using sampling with top_p=0.92 and top_k=0\n",
        "fine_tuned_model_output = fine_tuned_model.generate(\n",
        "    **base_model_inputs,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = fine_tuned_tokenizer.decode(fine_tuned_model_output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxpaK-QF-Yys"
      },
      "source": [
        "**Prompting**\n",
        "- Few Shot Promptiong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6A7IaAv-etn"
      },
      "outputs": [],
      "source": [
        "# Few-Shot Prompt\n",
        "few_shot_prompt = \"\"\"\n",
        "Question1: What is 6 - 4 ?\n",
        "Answer1: Answer is 2.\n",
        "\n",
        "Question2: Alex has 5 pens, and he buys 3 more pens. How many pens does Alex have now?\n",
        "Answer2: Alex now has 8 pens.\n",
        "\n",
        "Question3: Kim has 10 apples and gives 4 apples to Jung. How many apples does Kim have now?\n",
        "Answer3: Kim has 6 apples left.\n",
        "\n",
        "Quserion4: Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\n",
        "\n",
        "What is the answer of Question4?\n",
        "Answer4:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "base_model_few_shot_prompt = fine_tuned_tokenizer(few_shot_prompt, return_tensors=\"pt\").to(torch_device)\n",
        "\n",
        "# Generate response\n",
        "base_model_few_shot_output = fine_tuned_model.generate(\n",
        "    **base_model_few_shot_prompt,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "few_shot_generated_text = fine_tuned_tokenizer.decode(base_model_few_shot_output[0], skip_special_tokens=True)\n",
        "print(few_shot_generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6o3V1yU-1fE"
      },
      "source": [
        "**Prompting**\n",
        "- CoT(Chain of Thought) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eldQNd1Y-3fQ"
      },
      "outputs": [],
      "source": [
        "# COT Prompt\n",
        "cot_prompt = \"\"\"\n",
        "Question1: Sarah has 100 dollars. She buys a book for 40 dollars and a pencil for 5 dollars.\n",
        "How much money does Sarah have left?\n",
        "Answer1: Let's go through this step-by-step.\n",
        "Sarah starts with 100 dollars. After buying the book for 40 dollars, she has 100 - 40 = 60 dollars.\n",
        "Then, after buying the pencil for 5 dollars, she has 60 - 5 = 55 dollars. So, Sarah has 55 dollars left.\n",
        "\n",
        "Question2: John’s flight departs at 3:00 PM and arrives at 6:30 PM. How long is John’s flight?\n",
        "Answer2: Let's go through this step-by-step.\n",
        "John’s flight departs at 3:00 PM and arrives at 6:30 PM. From 3:00 PM to 6:00 PM is 3 hours.\n",
        "From 6:00 PM to 6:30 PM is 30 minutes. So, the total flight duration is 3 hours and 30 minutes.\n",
        "\n",
        "Question3: Kim has 3 boxes of apples, with each box containing 10 apples.\n",
        "Kim gives 1 box of apples to Jung and eats 3 apples from one of the remaining boxes.\n",
        "How many apples does Kim have now?\n",
        "\n",
        "What is the answer of Question3?\n",
        "Answer3:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "fine_tuned_model_cot_input = fine_tuned_tokenizer(cot_prompt, return_tensors=\"pt\").to(torch_device)\n",
        "\n",
        "# Generate response\n",
        "fine_tuned_model_cot_output = fine_tuned_model.generate(\n",
        "    **fine_tuned_model_cot_input,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "cot_generated_text = tokenizer.decode(fine_tuned_model_cot_output[0], skip_special_tokens=True)  # 출력도 base_model_cot_output 사용\n",
        "print(cot_generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
